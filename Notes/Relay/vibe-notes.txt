You said:
The Relay IR, a tensor-oriented, statically typed functional
IR, which we describe in Section 3. Relay‚Äôs design is motivated by the insight that functional IRs, used by languages
from the ML family1
can be readily adapted to support DL.
With its expressive semantics, including control flow, data
structures, and first-class functions, Relay can represent
entire state-of-the-art models.
‚Ä¢ The insight that common features in ML frameworks, such
as quantization and shape inference, can be reframed as
standard compiler passes. By using this reframing we can
tap into decades of traditional compilers research to design
composable optimization passes.
‚Ä¢ A platform-agnostic representation of operators and domain
specific optimizations which work in concert to provide
portability across hardware backends.

Could you explain what they bullets are proposing
ChatGPT said:
Got it ‚Äî let‚Äôs unpack those bullets step by step in plain terms:

1. Relay as a functional IR for deep learning
Relay is an intermediate representation (IR), basically a "middle language" that sits between high-level frameworks (like PyTorch or TensorFlow) and low-level machine code.

It borrows ideas from functional programming languages (like ML, Haskell, OCaml).

Unlike some earlier deep learning IRs that only describe computation graphs (e.g., TensorFlow graphs), Relay is more expressive: it supports things like control flow (loops, conditionals), data structures, and first-class functions.

This means it can represent not just fixed neural nets but also more dynamic, state-of-the-art models (e.g., ones with conditional execution or recursion).

üëâ Proposal: Treat deep learning programs as if they were functional programs, so you can express any model in a clear, structured way.

2. Reframing ML framework features as compiler passes
Many deep learning frameworks have custom implementations for things like:

Quantization (reducing precision for efficiency, e.g., FP32 ‚Üí INT8)

Shape inference (figuring out tensor dimensions automatically)

Instead of treating these as "special deep learning tricks," Relay views them as compiler transformations ‚Äî the same kind of optimizations compilers have been doing for decades (like constant folding, dead code elimination, type inference).

By framing them this way, Relay can leverage existing compiler theory to make these features modular and composable.

üëâ Proposal: Use the compiler mindset to unify and improve deep learning framework features, making them easier to extend and combine.

3. Platform-agnostic operators + domain-specific optimizations
One challenge in DL is running the same model on many hardware targets (GPUs, CPUs, TPUs, mobile chips, accelerators).

Relay defines operators and optimizations in a way that is platform-agnostic: the same Relay program can be lowered to different backends.

At the same time, it allows domain-specific optimizations (like fusing convolutions with batch norm) that can be applied regardless of the target hardware.

The result is portability + performance: you don‚Äôt have to rewrite models for every new device.

üëâ Proposal: Separate the model representation from the hardware so optimizations can be reused across platforms while still being specialized for each backend.

‚úÖ In short:
Relay‚Äôs bullets propose (1) using a functional programming‚Äìinspired IR to represent modern deep learning models, (2) reinterpreting ML framework features as compiler optimizations, and (3) designing operators/optimizations in a hardware-agnostic way to maximize portability and efficiency.

Would you like me to also draw a diagram-style explanation showing how Relay sits between DL frameworks and hardware backends?

You said:
Explain the difference between:

- Define And Run Frameworks
- Define By Run Frameworks
- Low Level Tensor Compilers
- Deep Learning Compilers
- Programming Languages for Deep Learning

What are the pros and cons of each
ChatGPT said:
Great question ‚Äî these terms often get mixed together, so let‚Äôs carefully break them down.

1. Define-and-Run Frameworks (a.k.a. static graph frameworks)
Definition: You define the entire computation graph (model) first, then run it.

Examples: TensorFlow 1.x, Theano, Caffe.

How it works: You write down the graph of operations (nodes and edges), then the framework compiles/optimizes it and executes.

‚úÖ Pros:

Optimizations are easier (since the whole graph is known upfront).

Efficient execution across devices (CPU, GPU, TPU).

Good for deployment (portable, optimized graphs).

‚ùå Cons:

Less intuitive for developers (harder debugging).

Limited support for dynamic models (loops, conditionals, recursion).

Slower iteration during model development.

2. Define-by-Run Frameworks (a.k.a. dynamic graph frameworks)
Definition: The graph is built as you run the code (imperative style).

Examples: PyTorch, TensorFlow Eager Mode, Chainer.

How it works: Each forward pass dynamically constructs the graph based on Python execution.

‚úÖ Pros:

Very intuitive and Pythonic (feels like normal programming).

Great for research, especially with dynamic models.

Easier debugging (since you can step through execution).

‚ùå Cons:

Harder to optimize (graph is not known ahead of time).

Sometimes less efficient for deployment.

May need tracing or conversion to static form for production.

3. Low-Level Tensor Compilers
Definition: Tools that generate optimized code for individual tensor operations (kernels).

Examples: cuDNN, MKL-DNN (oneDNN), TVM‚Äôs tensor expression language.

How it works: They focus on optimizing matrix multiplications, convolutions, etc., often hardware-specific.

‚úÖ Pros:

Extremely optimized performance for core tensor ops.

Exploit hardware features (SIMD, GPUs, cache locality).

Widely used in DL frameworks as building blocks.

‚ùå Cons:

Not model-aware ‚Äî they optimize ops, not graphs.

Portability can be limited (often vendor-specific).

Users rarely interact with them directly.

4. Deep Learning Compilers
Definition: Compilers that take a full deep learning model (graph) and optimize it end-to-end.

Examples: XLA (TensorFlow), Glow (Facebook), TVM/Relay, nGraph.

How it works: They analyze the entire computation graph and perform optimizations like operator fusion, memory planning, quantization, and hardware lowering.

‚úÖ Pros:

Whole-model optimizations (beyond single ops).

Portability across backends.

Automates tuning (less need for hand-written kernels).

‚ùå Cons:

More complex infrastructure.

May lag behind frameworks in supporting new ops.

Performance depends on maturity of backend support.

5. Programming Languages for Deep Learning
Definition: Full-fledged programming languages (or DSLs) designed with DL in mind, not just libraries.

Examples: Swift for TensorFlow (discontinued), Julia‚Äôs Flux, Relay (IR as a functional language), Mojo.

How it works: They embed DL concepts directly into the language (autograd, tensors, parallelism).

‚úÖ Pros:

Deep integration with compiler and type system.

Can unify training + deployment in one language.

Potential for both expressiveness (like PyTorch) and optimizability (like TensorFlow).

‚ùå Cons:

Ecosystem immaturity (smaller community).

Steeper learning curve (new syntax, semantics).

May lag in libraries/tools compared to Python-based frameworks.

Summary Table
Category	Scope	Examples	Pros	Cons
Define-and-Run Frameworks	Whole model (static graph)	TensorFlow 1.x, Theano	Optimizable, good deployment	Rigid, hard for dynamic models
Define-by-Run Frameworks	Whole model (dynamic graph)	PyTorch, Chainer	Flexible, intuitive, great for research	Harder to optimize, deployment tricky
Low-Level Tensor Compilers	Single operations (kernels)	cuDNN, oneDNN	Extremely fast ops, hardware-specific	Not model-aware, vendor lock-in
Deep Learning Compilers	Whole-model optimization	XLA, TVM, Glow	End-to-end optimization, portable	Complex, lagging op support
Programming Languages for DL	Language + IR + compiler	Relay, Julia/Flux, Swift4TF	Unified, expressive + optimizable	Immature ecosystems
üëâ In short:

Define-and-run = static, efficient, less flexible.

Define-by-run = dynamic, easy to use, less efficient.

Low-level tensor compilers = fast ops, not full models.

Deep learning compilers = optimize full models across hardware.

Languages for DL = push the boundaries by making DL a first-class citizen in a programming language.

Do you want me to also map out how these fit together in the DL software stack (like: Python framework ‚Üí DL compiler ‚Üí tensor compiler ‚Üí hardware)? That might make the relationships clearer.

You said:
Multiple Outputs Computation graph IRs have primitive support for multiple outputs because many tensor operators require it. For example, the split operator separates a tensor
along a given axis and returns each component. In Relay,
multiple outputs can be modeled as tuples, requiring only two
rules: tuple formation and tuple projection.

Explain what this means
ChatGPT said:
Great question ‚Äî let‚Äôs break this down.

1. Why multiple outputs matter in a computation graph
In deep learning, many operators don‚Äôt just produce a single tensor.

Example:

split(x, axis=1, sections=3) ‚Üí gives you 3 tensors.

max_pool2d(x, return_indices=True) ‚Üí gives both the pooled tensor and the indices.

So, the IR (Intermediate Representation) used to model computation graphs needs a way to represent multiple outputs from a single operation.

2. Primitive support in computation graph IRs
Many DL IRs (like TensorFlow graphs, ONNX) directly allow an operator node to have multiple output "edges."

E.g., one node can connect to several downstream nodes, each consuming one of the outputs.

That‚Äôs "primitive support": it‚Äôs baked into the graph structure.

3. Relay‚Äôs approach: tuples
Relay doesn‚Äôt introduce a special case for ‚Äúmultiple outputs.‚Äù

Instead, it reuses a general functional programming construct: tuples.

A tuple is just an ordered collection of values, like (y1, y2, y3).

To handle multiple outputs:

The operator returns a tuple containing all outputs.

To use one of them, you apply tuple projection (i.e., "pick the ith element").

So in Relay:

split(x) ‚Üí returns (y1, y2, y3) as a tuple.

If you need just the second piece, you do tuple_getitem(split(x), 1).

4. Why this is elegant
Relay only needs two simple rules in its semantics:

Tuple formation: how to create a tuple (v1, v2, ..., vn).

Tuple projection: how to extract the ith element from a tuple.

No need for special multi-output nodes in the IR.

This keeps the language minimal, clean, and general ‚Äî since tuples can represent not just multi-output ops, but also structured data.

‚úÖ In short:
Other DL IRs build special ‚Äúmulti-output‚Äù mechanics into the graph. Relay avoids that by saying:

"If an op has multiple outputs, just treat them as a tuple. We already know how to make and unpack tuples."

Would you like me to also show a side-by-side example (like split in ONNX vs. split in Relay) so you can see the structural difference?

You said:
What is Hindley-Milner-style type inference algorithm 
ChatGPT said:
Good question ‚Äî Hindley‚ÄìMilner (HM) type inference is one of the most influential ideas in programming languages. Let‚Äôs unpack it step by step.

1. The problem it solves
In statically typed languages, every expression has a type.

Usually, the programmer has to annotate types manually (e.g., int add(int x, int y) in C).

The question is: can the compiler figure out the types automatically without explicit annotations?

üëâ Hindley‚ÄìMilner provides an algorithm to do exactly that: infer the most general type for every expression.

2. Where it‚Äôs used
Functional languages like ML, OCaml, Haskell (without extensions), F# use it.

It‚Äôs also the inspiration behind modern type inference in Rust, Scala, Swift, and even Relay IR (from your earlier question).

3. Core idea
The HM algorithm works by:

Assigning type variables to every expression.

Example: if you have fn x -> x, assume x : Œ± and the function type is Œ± ‚Üí Œ±.

Generating constraints by looking at how expressions are used.

If you apply a function f e, then f must have type œÑ1 ‚Üí œÑ2 and e : œÑ1.

Unifying types (solving the constraints).

"Unification" is like solving equations between types, e.g., Œ± ‚Üí Œ≤ = int ‚Üí Œ≥ means Œ± = int and Œ≤ = Œ≥.

Generalizing types: once you finish a scope (like a let binding), replace type variables that are unconstrained with type schemes (‚àÄ quantifiers), making the function polymorphic.

Example: let id = fn x -> x ‚Üí type is ‚àÄŒ±. Œ± ‚Üí Œ± (the most general identity function).

4. Example
Expression:

let id = fn x -> x in
let f = fn y -> id y in
f 42
Step by step:

fn x -> x ‚Üí assign x : Œ±, so function is Œ± ‚Üí Œ±. Generalize: id : ‚àÄŒ±. Œ± ‚Üí Œ±.

fn y -> id y: apply id to y. Since id can take any type, y : Œ≤, so id y : Œ≤. Therefore, function is Œ≤ ‚Üí Œ≤.

f 42: apply f : Œ≤ ‚Üí Œ≤ to 42 : int. That forces Œ≤ = int.

Result type = int.

5. Properties
‚úÖ Pros:

Principal types: The algorithm always finds the most general type (not too specific, not too vague).

No need for annotations: Very convenient for programmers.

Polymorphism for free: Functions can work on many types (like generics).

‚ùå Cons:

HM works best for languages without complicated features (e.g., type classes, subtyping, higher-kinded types).

Many modern languages need extensions (like Haskell‚Äôs type classes or Rust‚Äôs traits), which go beyond pure HM.

6. One-liner definition
üëâ Hindley‚ÄìMilner is a type inference algorithm that uses unification to infer the most general (polymorphic) types for expressions in functional languages, without requiring explicit type annotations.

Would you like me to also sketch a mini step-by-step unification tree for a concrete example like (fn f -> fn x -> f x) to see how HM ‚Äúsolves the equations‚Äù?

You said:
Frontend There are several ways to write an Relay program.
A user can build an in-memory representation of a program
in C++ or Python, parse one written in the Relay text format, load one from the on-disk serialization format, or import
one from popular frameworks and interchange formats (e.g.,
TensorFlow, MxNet, Keras, DarkNet, and ONNX). Many
frameworks and interchange formats use static computation
graph-based representations, which can easily be translated
into Relay. A greater challenge is translating frameworks with
a richer computation model such as TensorFlow (TF). TF supports control flow and includes TensorArray, a write-once
tensor container. We can extract the loop structure out of the
TF graph, converting it to an Relay loop, and transform the
TensorArray into an Relay list. Once new deep learning
languages and IRs under development are stable it is likely
they can be translated into Relay (see Section 2.4). PyTorch
provides an expressive programming model, and is a good fit
for Relay, which has integration into PyTorch‚Äôs JIT infrastructure, enabling users to transparently use Relay for improved
performance.
Compiler Once an Relay abstract syntax tree (AST) is produced, the program is optimized by applying a series of Relayto-Relay passes. Between each pass, Relay performs type
inference and checking, rejecting malformed programs as well
as populating shape and type information that passes can utilize. The Relay compiler supports traditional optimizations
(e.g., constant folding, common subexpression elimination,
and dead code elimination) and domain-specific optimizations
(see Sec. 4).
Backends Relay produces machine-specific code by decomposing the problem of code generation into multiple distinct
phases. Relay translates all operators into TVM expressions
to produce dense linear algebra kernels [7, 53, 35]. TVM
produces low-level operators that expect a fixed calling convention, as well as preallocated inputs and outputs. The result
is an object file containing hardware-specific implementations
of all operations. The remaining Relay program then is executed or compiled, with operator invocations replaced by calls
to the optimized operators. By representing operators as TVM
expressions, we can programmatically transform them and automatically generate new implementations for the transformed
operators. Optimizations like fusion and quantization rely on
this novel behavior. After primitive operators are lowered, the
remaining Relay program ties together operator invocations,
allocation, control-flow, recursion, and high-level data structures. There are multiple options for executing the combined
full program: the Relay interpreter (with JIT compilation), an Relay virtual machine, the TVM graph runtime, and an experimental Relay ahead-of-time compiler that converts programs 
to C++ to produce a target-specific binary.

Explain what each stage is doing
ChatGPT said:
Perfect ‚Äî this excerpt describes the end-to-end compiler pipeline of Relay (inside TVM). Let‚Äôs walk through each stage step by step.

1. Frontend (Getting models into Relay)
This stage is about input formats ‚Üí Relay IR.
Relay provides multiple ways to construct a program:

Direct construction: You can build a Relay AST (abstract syntax tree) in C++ or Python.

Parsing text: Write Relay code in its text format, then parse it.

Loading serialized Relay: Load pre-saved IR from disk.

Importing from other frameworks: Translate models from TensorFlow, MXNet, Keras, DarkNet, or ONNX.

üëâ The challenge:

Frameworks like ONNX are static graphs, so they map easily into Relay‚Äôs IR.

Frameworks like TensorFlow or PyTorch have richer semantics (loops, conditionals, dynamic arrays). Relay must carefully translate them:

TensorFlow loops ‚Üí Relay loops.

TensorArray ‚Üí Relay lists.

PyTorch JIT ‚Üí integrates directly, so Relay can optimize PyTorch models transparently.

‚úÖ Purpose: Take models from any source and get them into a uniform IR (Relay).

2. Compiler (Relay-to-Relay optimization)
Once we have a Relay AST, we apply a sequence of optimization passes.

After each pass, Relay performs type inference and checking to:

Ensure the program is still valid.

Infer shapes and types that later passes can use.

General compiler optimizations:

Constant folding (compute constants at compile time).

Common subexpression elimination.

Dead code elimination.

Domain-specific optimizations (DL-focused):

Operator fusion (combine multiple ops into one kernel).

Quantization (reduce precision for performance).

Layout transformations, etc.

‚úÖ Purpose: Improve performance and simplify the IR while keeping semantics the same.

3. Backends (Lowering Relay to hardware code)
This is where Relay ‚Üí TVM ‚Üí machine code happens.

Operator lowering:

Each Relay operator (like conv2d, matmul) is translated into a TVM expression.

TVM then generates optimized low-level kernels (dense linear algebra ops).

The result is hardware-specific object code (e.g., CUDA for GPUs, LLVM IR for CPUs).

Program lowering:

After primitive operators are compiled, the remaining Relay program ties them together:

Control flow (loops, conditionals).

Data structures (lists, tuples).

Recursion, memory allocation.

Execution options:
Relay can run the final program in different ways:

Relay interpreter (with JIT): Run directly with on-the-fly compilation.

Relay VM: A virtual machine executing Relay bytecode.

TVM graph runtime: Executes Relay as a static computation graph.

Relay ahead-of-time (AOT) compiler: Converts Relay into C++ and produces a binary specific to the target.

‚úÖ Purpose: Turn the optimized Relay IR into hardware-specific executables for CPUs, GPUs, TPUs, or accelerators.

Summary (The Pipeline)
Frontend ‚Üí Translate models (TF, PyTorch, ONNX, etc.) into Relay IR.

Compiler ‚Üí Apply type inference + optimizations (both classic compiler ones and DL-specific).

Backends ‚Üí Lower operators into TVM kernels, stitch them together, and choose an execution mode.

‚ö° In short:
Relay acts as the unified high-level IR for deep learning:

Frontend: ingest models,

Compiler: optimize,

Backend: lower to efficient hardware code.

Would you like me to also draw this as a flow diagram (Frontend ‚Üí Compiler ‚Üí Backend ‚Üí Execution) so the roles are visually clearer?

You said:
What is Operator fusion?
ChatGPT said:
Great question ‚Äî operator fusion is one of the most important optimizations in deep learning compilers like TVM/Relay, XLA, and Glow.

1. The basic idea
Operator fusion means combining multiple operations into a single kernel so they can run together, instead of separately.

Example:

y = ReLU(MatMul(x, W) + b)
Naively, this involves 3 separate kernels:

Matrix multiply MatMul(x, W)

Add bias + b

Apply ReLU

With fusion, these are merged into one kernel:

The kernel computes the matmul, adds the bias, and applies ReLU all in one pass.

2. Why this is beneficial
Reduce memory traffic:

Without fusion: intermediate results (e.g., the matmul output) are written to memory, then read again by the next op.

With fusion: intermediate values stay in registers or cache, avoiding expensive memory writes/reads.

Lower kernel launch overhead:

On GPUs, launching many small kernels has overhead.

Fusion reduces the number of kernel launches.

Better optimization opportunities:

Fused kernels can use vectorization, tiling, and hardware-specific optimizations across ops.

3. Types of fusion
Elementwise fusion:
Combine a sequence of elementwise ops (e.g., y = sigmoid(x + b) ‚Üí one kernel).

Producer‚Äìconsumer fusion:
Fuse a compute-heavy op with its immediate consumer (e.g., MatMul + ReLU).

Horizontal fusion:
Combine multiple independent ops that can be done together (e.g., applying two elementwise ops in parallel on the same input).

4. Example in practice
Without fusion:

MatMul(x, W) ‚Üí temp
Add(temp, b) ‚Üí temp2
ReLU(temp2) ‚Üí y
Each step writes results to GPU memory.

With fusion:

FusedKernel(x, W, b) ‚Üí y
Compute matmul.

Immediately add bias while values are still in registers.

Immediately apply ReLU.

Write final output once.

This reduces 3 memory writes + 3 memory reads ‚Üí 1 write.

5. Tradeoffs / challenges
‚úÖ Pros:

Significant speedup (especially on memory-bound workloads).

Less memory usage.

Closer to hardware peak performance.

‚ùå Cons:

Harder to schedule (need to balance fusion with parallelism).

Too much fusion can lead to giant kernels that don‚Äôt fit cache/registers.

Compiler complexity increases (finding the best fusion strategy is nontrivial).

üëâ In short:
Operator fusion = merging multiple operators into one kernel to reduce memory traffic, kernel launches, and improve performance. It‚Äôs one of the key tricks that makes deep learning compilers faster than na√Øve framework execution.

Would you like me to show a Relay/TVM example of operator fusion (before/after IR code) so you can see how it looks in practice?

You said:
Operator fusion is an indispensable optimization in deep learning compilers. Fusion enables better sharing of computation,
removal of intermediate allocations, and facilitates further optimization by combining loop nests. Fusion is known to be the
most critical optimization in machine learning compilers, but
existing fusion techniques are closed (working over a fixed set
of ops) and target-dependent. Traditional operator fusion algorithms resemble instruction selection: A sequence of operators
eligible for fusion is first identified and then replaced with
a corresponding handwritten fused implementation, usually
from a vendor-provided library. For example, if a fused implementation for a GPU operator does not exist in CuDNN, it
will remain unfused. More advanced strategies, implemented
in XLA, detect a closed set of statically shaped operators for
fusion and generate code for CPU/GPU.
Relay‚Äôs fusion algorithm addresses weaknesses of previous
approaches by representing all operators in a secondary IR.
Relay operators are backed by a TVM compute expression that
describes operations in a high-level DSL that resembles Einstein notation but omits low-level scheduling details. TVM‚Äôs
separation of compute and scheduling provides many favorable
qualities for Relay‚Äôs fusion algorithm. It enables producing
shape-specialized fused operators for an open set of operators,
fusing arbitrary-length chains of operators (not just pairwise
combinations), and handling operators with multiple outputs
and nonlinear consumer-producer patterns. TVM is also able
to reschedule after fusion and perform further optimization
via auto-tuning. Relay performs fusion in two steps, detailed
below


Above is an excerpt from the paper

Finds operators that can be "fused" and marks them

ex. y = ReLU(mx+b)

1. temp = m * x
2. temp1 =  temp1 + b
3. y = ReLU(temp1)

Combine these into:

y = FusedKernel(x, m, b)


Accomplishes this by representing ALL operators in secondary IR, allows lower level tools on the compiler toolchain to choose what to fuse based on hardware. 

Is above a good explanation of the excerpt?
ChatGPT said:
Yes ‚Äî your explanation captures the spirit of the excerpt very well, but let‚Äôs sharpen it a bit so it lines up more precisely with what the paper is saying.

‚úÖ What you got right
Fusion example: Your example with y = ReLU(mx + b) is spot on ‚Äî it shows how multiple sequential operators can be replaced with one fused kernel.

Goal of fusion: You correctly mention reducing intermediate allocations and letting compilers decide how to fuse based on hardware.

Secondary IR: You note that Relay represents operators in a secondary IR, so lower-level compiler tools (TVM) can handle actual code generation and scheduling.

‚ö†Ô∏è What can be clarified
Difference from older approaches

Previous compilers (like TensorFlow with cuDNN) only fused when a handwritten fused kernel existed for the target device.

Relay doesn‚Äôt need prewritten fused kernels ‚Äî instead, it can generate fused kernels automatically via TVM‚Äôs compute DSL.

Open vs closed set of operators

Older compilers had a closed set of fusion patterns (e.g., only specific op sequences).

Relay supports an open set of operators: any sequence can potentially be fused, because fusion is expressed at a higher level.

Handling complex cases

Relay can fuse not only straight-line chains (like your example), but also:

Arbitrary-length chains.

Multi-output operators.

Nonlinear consumer‚Äìproducer graphs (not just simple linear ones).

TVM‚Äôs role

Relay handles fusion marking (finding groups of ops).

TVM handles scheduling and tuning of those fused operators, which makes them hardware-specialized.

‚ú® Refined short explanation
Relay‚Äôs operator fusion improves on older approaches by representing all operators in a high-level IR (via TVM). Instead of relying on a fixed set of prewritten fused kernels (as in cuDNN or XLA), Relay can fuse arbitrary operator chains, including complex cases with multiple outputs, and automatically generate optimized fused implementations. This separation of compute (Relay/TVM IR) from scheduling (TVM auto-tuning) makes fusion hardware-aware and extensible.

üëâ So your explanation is good ‚Äî but if you emphasize the open-set fusion + automatic generation vs fixed handwritten kernels distinction, it will be fully faithful to the excerpt.

Would you like me to rewrite your explanation into a tight 3‚Äì4 sentence summary that could serve as a clean ‚Äútakeaway‚Äù version of the paper‚Äôs fusion contribution?

