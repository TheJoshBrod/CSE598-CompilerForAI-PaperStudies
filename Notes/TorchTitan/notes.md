TorchTitan: One-stop PyTorch native solution for production ready LLM pre-training

## Abstract

How do we train giant models efficiently without reinventing the wheel every time?

- Huge compute: Thousands of GPUs
- Complex Distributed Systems: Mixing data, tensor, pipeline, and sequence parallelism
- Engineering Effort: Variety of tools across repos that are hard to combine and maintain

TorchTitan is:
- Integrates with PyTorch
- One unified system
- Modular for mixing and matching parallelism strategies
- Elastically adapts to available hardware
- Logger, checkpoint-er, debugger

## Introduction

Training new models is difficult due to balancing parallelism, computation, and communication with navigating trade off between memory and computation

This can cause GPU failure.

### To work around this the system must be 4D parallel:
- Data Parallel: Replicate full model on many GPUs with input mini batch, syncing gradient
- Tensor Parallel: Splits computation across multiple GPUs (ex. partition large matmul)
- Context Parallel: Splits sequence length dimension across devices 
- Pipeline Parallel: Split model layers into "stages" with mini-batches flow into pipeline

### Combine the system with optimizations:

- Activation Recomputation: Instead of storing all intermediate activations from the forward pass, selectively drop some and recompute them during the backward pass to save memory, tradeoff compute

- Mixed Precision Training: Runs most computations in lower precision (FP16, BF16, or FP8) for faster training and reduced memory usage, while keeping critical values (like master weights, loss scaling, reductions) in FP32 to maintain stability and accuracy.

- Deep Learning Compilers: Use Frameworks (like TorchInductor, XLA, TVM) that optimize model execution by Operator fusion, Graph-level optimizations, etc.

### Existing SOTA Distributed Training Techniques fall short:
- Non-Composable: Struggle to integrate and stack parallelism techniques
- Inflexible Architecture: Lacks modularity of new techniques, optimizations, and hardware
- Inefficient Hardware Util: Bad usage of advanced GPU features = bad efficiency
- Bad for Prod Training: Limited checkpoint scaling, hard to recover, and bad debugging
- Framework Limits: Rely on external tools, fail to use PyTorch optiml kernels, new features, or compiler support

### How does TorchTitan seek to solve this?
- Advance DTensor to allow sharding to support n-D parallelism, compatibility with torch.compile, and enable efficient n-D checkpointing via state support
- Demonstrates parallelism techniques to help multidimensional parallelism in LLM training
- Hardware/Software co-designed solution: exploits advanced hardware features
- Scalable training checkpoints w/ debugger, crash reports, and logging
- Tests TorchTitan on Llama 3.1 with various parallelism, parameter counts, and # of GPUs
- Created guidelines and training recipes

## Elasticity through composability

To keep things modular and parallel they separate:
- Model Definition: Parallelism agnostic and designed for readability
- Parallelism Helpers: Applies parallelism and training optimizations to model
- Generalized Training Loop: 

FSDP2: Took FSDP1 and adapted it as a technique to apply 1D data parallelism
- Shards model params, gradients, and optimizers
- Reduce memory footprint of model states

Rowwise Parallel and Sequence Parallel: Tensor/Sequence Parallelism
- Shareds the math of large linear layers
- Tensor: Reduces compute/memory of BIG operations inside a layer
- Sequence: Shards computation for normalization and drop out layers
- Loss parallel shards the loss calculations

Pipeline Parallelism:
- Separates each section of the model into different stages of a pipeline
- Each stage receives input activations, computes locally and sends output activations 
- Last stage computes gradients and passes backwards
- Input batch is split into micro-batches and schedule overlaps computation and communication

Context Parallelism:
- Splitting the sequence dimension across GPUs to increase context length

## Optimizing Training Efficiencies in TorchTitan

Activation Checkpointing/Selecting Activation Checkpointing (AC/SAC)
- Reduces peak GPU memory by trading recomputation over memory saving

Regional Compilation:
- Compiles torch per transformer block to avoid graph breaks, allow it to be distributed, and only compile repeated blocks once to gain throughput and memory efficiency

Async Tensor Parallel (AsyncTP):
Improves Tensor Parallelism by chunking large computations and overlapping them with communication (micro-pipelining), while PyTorchâ€™s SymmetricMemory enables faster GPU-to-GPU transfers, and TorchTitan integrates this seamlessly with torch.compile for meaningful speedups on H100-class hardware

Mixed Precision & Float8 Training:
- Supports training on Float8 

## Production-Ready Training in TorchTitan

Scalable and efficient Distributed Checkpointing:
- Unsharded checkpoints are reusable but expensive while sharded are fast but harder to reuse
- DCP uses DTensor to store shard and global info in parallelism agnostic format

Flight Recorder to Debug Job Crashes
- Logs start, end, and enqueue time for all NCCL functions